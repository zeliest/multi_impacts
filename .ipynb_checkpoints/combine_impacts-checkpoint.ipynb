{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cdf1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.util.api_client import Client\n",
    "from climada.util.constants import RIVER_FLOOD_REGIONS_CSV\n",
    "from climada_petals.entity.impact_funcs.river_flood import ImpfRiverFlood,flood_imp_func_set\n",
    "import pandas as pd\n",
    "from climada.engine import Impact\n",
    "from climada.hazard import Hazard\n",
    "import numpy as np\n",
    "from climada.util import yearsets\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d5309a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_events(impact, lam=1, years = np.array(range(1980,2000))):\n",
    "    #create sampling vector\n",
    "    events_per_year = yearsets.sample_from_poisson(len(np.array(range(1980,2000))), lam)\n",
    "    sampling_vect = yearsets.sample_events(events_per_year, impact.frequency)\n",
    "    impact_sample = yearsets.impact_from_sample(impact, years, sampling_vect)\n",
    "    return impact_sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a749cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['VNM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fb62892",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_csvs = {}\n",
    "impact_mats = {}\n",
    "\n",
    "for country in countries:\n",
    "\n",
    "    impact_csvs[(country,'RF')] = os.path.join(data_path, \"\".join(['RF_impact_mat/river_flood_impact_150arcsec_historical_1980_2000_',country,'.csv']))\n",
    "    impact_mats[(country,'RF')] =os.path.join(data_path, \"\".join(['RF_impact_mat/river_flood_impact_150arcsec_historical_1980_2000_',country,'.csv']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "369021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_yearset_country(country, csv, impact_mat, n_samples=100, years=np.array(range(1980,2000))):\n",
    "    impact = Impact.from_csv(csv)\n",
    "    impact.imp_mat = impact.read_sparse_csr(impact_mat)\n",
    "    lam = np.sum(impact.frequency)\n",
    "    yearset_list = [sample_events(impact, lam=lam) for n in range(n_samples)]\n",
    "    if yearset_list[0].imp_mat.shape[0]>len(np.array(range(1980,2000))):\n",
    "        client =Client()\n",
    "        exposures = client.get_litpop_default('vietnam')\n",
    "        yearset_list = [yearset.aggregate_impact_to_year(yearset, exp=exposures) for yearset in yearset_list]\n",
    "    return(yearset_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518db90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a94ec08d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f5/lms0b90j1kx7kk4v552qkjqw0000gp/T/ipykernel_96898/4236308581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m impacts_yearsets = {country: make_yearset_country(country, impact_csvs[country], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                   impact_mats[country]) for country in impact_csvs}\n",
      "\u001b[0;32m/var/folders/f5/lms0b90j1kx7kk4v552qkjqw0000gp/T/ipykernel_96898/4236308581.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m impacts_yearsets = {country: make_yearset_country(country, impact_csvs[country], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                   impact_mats[country]) for country in impact_csvs}\n",
      "\u001b[0;32m/var/folders/f5/lms0b90j1kx7kk4v552qkjqw0000gp/T/ipykernel_96898/4128915257.py\u001b[0m in \u001b[0;36mmake_yearset_country\u001b[0;34m(country, csv, impact_mat, n_samples, years)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_yearset_country\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpact_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myears\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1980\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimpact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImpact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimpact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimp_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimpact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sparse_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpact_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0myearset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_projects/climada_python/climada/engine/impact.py\u001b[0m in \u001b[0;36mread_sparse_csr\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \"\"\"\n\u001b[1;32m    743\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         return sparse.csr_matrix(\n\u001b[1;32m    746\u001b[0m             (loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
      "\u001b[0;32m~/miniconda3/envs/climada_env/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    446\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "impacts_yearsets = {country: make_yearset_country(country, impact_csvs[country], \n",
    "                                                  impact_mats[country]) for country in impact_csvs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54080a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
